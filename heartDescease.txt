# -*- coding: utf-8 -*-
import numpy as np 
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt

#ignorer les avertissements
import warnings
warnings.filterwarnings('ignore')


class svm:
    # configuration de base et initialisation des poids et biais
    def __init__(self, learning_rate=0.001, lambda_param=0.01, nbr_iters=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.nbr_iters = nbr_iters
        self.w = None
        self.b = None

    def _init_weights_bias(self, x):
        n_features = x.shape[1]
        self.w = np.zeros(n_features)
        self.b = 0

    # Mappez les étiquettes de classe de {0, 1} à {-1, 1}
    def _get_cls_map(self, y):
        # if y=0 then map to -1
        return np.where(y <= 0, -1, 1)

    # vérifier si la contrainte de l'hyperplande séparation est satisfaite
    def _satisfy_constraint(self, x, idx):
        linear_model = np.dot(x, self.w) + self.b  # w*x+b
        return self._get_cls_map[idx] * linear_model >= 1  # y(w*x+b)>=1

    def _get_gradients(self, constrain, x, idx):
        # si le point de données se trouve du bon coté
        if constrain:
            dw = self.lambda_param * self.w  # lambda*w
            db = 0
            return dw, db

        # si le point de données est du mauvais coté
        dw = self.lambda_param * self.w - np.dot(self._get_cls_map[idx], x)  # lambda*w - y*x
        db = - self._get_cls_map[idx]
        return dw, db

        # mise à jour les paramétres en conséqunence

    def _update_weights_bias(self, dw, db):
        self.w -= self.lr * dw  # w = w- learning_rate *dw
        self.b -= self.lr * db  # w = w- learning_rate *db

    def fit(self, x, y):
        # init le poids et le biais
        self._init_weights_bias(x)
        # mapper la classe binaire sur {-1,1 }
        self._get_cls_map = self._get_cls_map(y)

        for _ in range(self.nbr_iters):
            for idx, x in enumerate(x):
                # vérifier si le point de données satisfait la contrainte
                constrain = self._satisfy_constraint(x, idx)
                # claculer le gradients en conséquence
                dw, db = self._get_gradients(constrain, x, idx)
                # mettre à jour les poids et les biais
                self._update_weights_bias(dw, db)

    def predict(self, x):
        estimate = np.dot(x, self.w) + self.b

        # calculer le signe des étiquettes de classe
        prediction = np.sign(estimate)

        # mapper la classe de {-1 , 1} aux valeurs d'origine {0,1 } avant de renvoyer l'étiqutte prédite
        return np.where(prediction == -1, 0, 1)


#convertir en liste:
def convert_data(df) -> list:
	r = []
	for index, line in df.iterrows():
		obs = [
			float(line['SepalLengthCm']),
			float(line['SepalWidthCm']),
			float(line['PetalLengthCm']),
			float(line['PetalWidthCm']),

		]
		r.append(obs)

	return r


def main():
    #les paramètres
    pd.set_option('display.max_columns', None)

    #Lire et afficher la base de données
    df = pd.read_csv('/Users/pc/Desktop/dataset.csv')
    df

    # afficher les dimensions de DataFrame
    print("dimension de DataFrame: {} rows, {} columns".format(df.shape[0], df.shape[1]))

    # énumération des colonnes
    print(df.columns,"\n")
    """
    Les données des colonnes contiennent :
    
        age - âge en années .
        sex - (1 = masculin ; 0 = féminin) .
        cp - type de douleur thoracique .
        trestbps - tension artérielle au repos (en mm Hg à l'admission à l'hôpital) .
        chol - sérum cholestoral en mg/dl .
        fbs - (glycémie à jeun > 120 mg/dl) (1 = vrai ; 0 = faux) .
        restecg - résultats électrocardiographiques au repos .
        thalach - fréquence cardiaque maximale atteinte .
        exang - angine induite par l'exercice (1 = oui ; 0 = non) .
        oldpeak - Dépression ST induite par l'exercice par rapport au repos .
        slope - la pente du segment ST d'exercice maximal . 
        ca - nombre de vaisseaux principaux (0-3) colorés par fluoroscopie .
        thal - 3 = normal ; 6 = défaut corrigé ; 7 = défaut réversible .
        target - être malade ou non (1=oui, 0=non) .
    """

    # imprimer les informations sur le DataFrame,
    # y compris le type d'index et les colonnes, les valeurs non nulles et l'utilisation de la mémoire
    df.info()

    # Générer des statistiques descriptives
    df.describe()

    # Obtenir des corrélations de chaque fonctionnalité dans l'ensemble de données
    corrmat = df.corr()
    top_corr_features = corrmat.index
    plt.figure(figsize=(12,12))
    #plot heat map
    g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlBu")

    # visualiser la distribution de données numériques
    df.hist( figsize=(14,12))
    """
    # C'est toujours une bonne pratique de travailler avec un ensemble de données
    # où les classes cibles sont de taille approximativement égale.
    # Ainsi, vérifions la même chose.
    """
    # Renvoie le nombre de valeurs uniques de chaque classe de target
    print(df['target'].value_counts(),'\n')
    # Renvoie le pourcentage des patients dans chaque classe
    countNoDisease = len(df[df.target == 0])
    countHaveDisease = len(df[df.target == 1])
    print("Pourcentage de patients n'ayant pas de maladie cardiaque: {:.2f}%".format((countNoDisease / (len(df.target))*100)))
    print("Pourcentage de patients atteints d'une maladie cardiaque: {:.2f}%".format((countHaveDisease / (len(df.target))*100)))

    sns.set_style('whitegrid')
    sns.countplot(x='target',data=df)
    plt.title('Répartition des classes de target')
    plt.xlabel('Maladie cardiaque')
    plt.ylabel('Nombre')
    plt.show()

    '''
    Pour les variables catégorielles, nous devons créer des variables factices.
    Je vais également laisser tomber la première catégorie de chacun. 
    Par exemple, plutôt que d'avoir 'masculin' et 'féminin', nous aurons 'masculin' avec des valeurs de 0 ou 1 
    (1 étant masculin, et 0 étant donc féminin).
     j'utiliserai la méthode get_dummies pour créer des colonnes factices pour les variables catégorielles.
    '''
    dataset = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])
    dataset

    '''
    Pour les autres variables qui ont de grandes valeurs, nous faisons une fonction log+1 pour réduire
    '''
    #prétraitement normalisation
    def log_trns(df, col):
        return df[col].apply(np.log1p)

    log_lst = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
    for col in log_lst:
        dataset[col] = log_trns(dataset, col)

    dataset

    # Mélanger les données
    '''
    Puisque nous voulons mélanger tout le DataFrame,
    nous utilisons la méthode sample () dans les panda avec frac=1 pour que tous les enregistrements soient renvoyés 
    (si on travaille avec des tableaux numpy au lieu de dataframe on utilise la methode np.random.shuffle(dataset) )
    '''
    dataset = dataset.sample(frac = 1)
    dataset

    # Extraction des données
    y = dataset['target']
    X = dataset.drop(['target'], axis = 1)

    # division des données
    # 80 % de nos données seront des données de train et 20 % seront des données de test.
    div_index = int(0.80 * len(X))
    x_train = X[0:div_index]
    y_train = y[0:div_index]
    x_test = X[div_index:]
    y_test = y[div_index:]

    print(x_train)
    svm1 = svm()
    #svm1.fit(x_train, y_train)


main()


